{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiMprgThJ0ts",
    "nbpresent": {
     "id": "cddedaf0-78f1-467a-a38e-d7f337017620"
    }
   },
   "source": [
    "# Imports and Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fX1daDvdptIN",
    "nbpresent": {
     "id": "45196f26-9fea-4224-8fda-c0779e3899c6"
    }
   },
   "outputs": [],
   "source": [
    "path = 'D:\\DLProject\\RRNProject'\n",
    "output_path = 'D:\\DLProject\\RRNProject\\output'\n",
    "\n",
    "%run tools/imports.py\n",
    "%run -i tools/functions.py\n",
    "%run -i tools/models.py\n",
    "%run -i tools/phrasecut.py\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy94hXcCJqA5",
    "nbpresent": {
     "id": "ad2a1e77-d04e-4a98-adf5-5251b31789b8"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "opt={}\n",
    "opt = dotdict(opt)\n",
    "\n",
    "## General\n",
    "opt.dataset = 'phrasecut'    # 'phrasecut'\n",
    "opt.split = 'test'           # 'train' 'test'\n",
    "opt.test_iter = 1500\n",
    "opt.test_log_every = 50      # num of iterations to log test info\n",
    "opt.save_im_every = 500      # num of iterations to save mask output\n",
    "opt.checkpoint = 'D:\\DLProject\\RRNProject\\output\\checkpoint_100000.pth' # path to .pth to continue training\n",
    "\n",
    "## Hyperparams\n",
    "opt.phrasecut_categories = ['c_coco'] # filter categories\n",
    "opt.new_img_proc = True               # replace old image proc with new\n",
    "opt.dcrf = True                       # DCRF post-processing\n",
    "opt.im_h = 320\n",
    "opt.im_w = 320\n",
    "opt.vf_h = 40\n",
    "opt.vf_w = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqTZ0X-Pr8Sh"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "opt.test_acc = {'iter': 'val'}\n",
    "opt.test_acc_pos = {'iter': 'val'}\n",
    "opt.test_acc_neg = {'iter': 'val'}\n",
    "opt.test_miou = {'iter': 'val'}\n",
    "opt.test_overall_iou = {'iter': 'val'}\n",
    "opt.test_miou_dcrf = {'iter': 'val'}\n",
    "opt.test_overall_iou_dcrf = {'iter': 'val'}\n",
    "opt.vocab_size = 8407\n",
    "\n",
    "# Load pre-trained cfg\n",
    "checkpoint = torch.load(opt.checkpoint)\n",
    "train_opt = checkpoint['opt']\n",
    "train_opt = dotdict(train_opt)\n",
    "\n",
    "# Initialize Model\n",
    "model = Model(train_opt)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "# activate GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == 'cuda':\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "# Load Data\n",
    "refvg_loader = RefVGLoader(split=opt.split)\n",
    "img_ref_data = refvg_loader.get_img_ref_data()\n",
    "task_i = -2\n",
    "print('Loaded phrasecut: %s images, %s tasks' % (len(refvg_loader.img_ids), refvg_loader.task_num))\n",
    "\n",
    "# Init Vocab\n",
    "with open(str(dataset_dir) + '/name_att_rel_count.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "corpus = Corpus()\n",
    "corpus.split_and_add_words_to_vocab_from_data(data)\n",
    "\n",
    "image_batch = np.zeros((1, opt.im_h, opt.im_w, 3), dtype=np.float32)\n",
    "\n",
    "# Acc and loss initialize\n",
    "running_acc, running_acc_pos, running_acc_neg, running_miou, running_overall_iou, I, U = 0, 0, 0, 0, 0, 0, 0\n",
    "running_miou_dcrf, running_overall_iou_dcrf, I_dcrf, U_dcrf, down_I, down_U = 0, 0, 0, 0, 0, 0\n",
    "bw, n_batch= 0, 0\n",
    "iou_metrics = [.5, .6, .7, .8, .9]\n",
    "iou_precision = np.zeros(len(iou_metrics), dtype=np.int32)\n",
    "dcrf_iou_precision = np.zeros(len(iou_metrics), dtype=np.int32)\n",
    "\n",
    "\n",
    "#im preprocess\n",
    "if opt.new_img_proc:\n",
    "    preprocess = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "else:\n",
    "    mu = np.array((104.00698793, 116.66876762, 122.67891434))\n",
    "\n",
    "# init test\n",
    "model.eval()\n",
    "\n",
    "for iter in tq.tqdm(range(opt.test_iter)):\n",
    "    \n",
    "    ############## Load Data ##############\n",
    "    \n",
    "    # Read next task\n",
    "    match = 0\n",
    "    while(match == 0):\n",
    "        task_i += 1\n",
    "        if (task_i >= len(img_ref_data['task_ids'])) or (task_i == -1):\n",
    "            img_ref_data = refvg_loader.get_img_ref_data()          # load img\n",
    "            img_id = img_ref_data['image_id']\n",
    "            img_p = str(img_fpath) + '/' + str(img_id) + '.jpg'     # (original shape) image\n",
    "            img = Image.open(img_p)\n",
    "            img = img.resize((opt.im_h, opt.im_w))                  # (320, 320) image\n",
    "            image = np.array(img).astype(np.float32)                # (320, 320, 3) np float\n",
    "            ubyte_im = skimage.img_as_ubyte(image.astype(np.uint8)) # (320, 320, 3) np uint8\n",
    "            task_i = 0\n",
    "\n",
    "        # get task and categories\n",
    "        task_ids = img_ref_data['task_ids']\n",
    "        task = task_ids[task_i]\n",
    "        subsets_of_img = refvg_loader.get_task_subset(img_id, task)\n",
    "        \n",
    "        # Filter data by category\n",
    "        if len(set(opt.phrasecut_categories).intersection(subsets_of_img)) == len(opt.phrasecut_categories):\n",
    "            match = 1\n",
    "\n",
    "    # extract phrase     \n",
    "    sentence = img_ref_data['phrases'][task_i]     #string 'large picture'\n",
    "    text_pass = corpus.tokenize_sentence(sentence).type(torch.LongTensor) #need to be (1,1,20) torch\n",
    "\n",
    "    # Ground truth mask\n",
    "    original_h = img_ref_data['height']            # 600\n",
    "    original_w = img_ref_data['width']             # 800\n",
    "    mask = np.zeros((original_h, original_w))                        # (600,800) np\n",
    "    gt_Polygons = img_ref_data['gt_Polygons'][task_i]                # [plg0, plg1,..] for seperate objects\n",
    "    for plg in gt_Polygons:\n",
    "        mask += polygons_to_mask(plg, w=original_w, h=original_h)    #(600,800) np 1/0\n",
    "    mask_up = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)    #(1,1,600,800) torch 1/0\n",
    "    #(1,1,320,320) torch 0/1\n",
    "    mask_pass = nn.functional.interpolate(mask_up, size=(320, 320), mode='bilinear', align_corners=False).to(device)\n",
    "\n",
    "    # Skip for black/white\n",
    "    if len(image.shape) == 2:\n",
    "        bw += 1\n",
    "        continue\n",
    "    \n",
    "    # processing image before pass\n",
    "    image_flip = image[:,:,0:3]         #rgba\n",
    "    image_flip = image_flip[:,:,::-1]   #(320, 320, 3)\n",
    "    if opt.new_img_proc != True:\n",
    "        image_flip -= mu\n",
    "\n",
    "    # add batch_size dimension\n",
    "    image_batch[n_batch, ...] = image_flip #(1, 320, 320, 3)\n",
    "\n",
    "    # turn into tensor\n",
    "    image_pass = torch.from_numpy(image_batch).permute(0,3,1,2).to(device) #(1,3,320,320) torch\n",
    "\n",
    "    # normalize [0,1] => normalize mean, std\n",
    "    if opt.new_img_proc:\n",
    "        image_pass = preprocess(image_pass.view(3,opt.im_h,opt.im_w)/255).view(1,3,opt.im_h,opt.im_w) #(1,3,320,320)\n",
    "        \n",
    "    ############## Test Step ##############\n",
    "    \n",
    "    # forward pass\n",
    "    output_down, output_up = model(image_pass, text_pass)  # output: (1,1,40,40), output_up: (1,1,320,320) pre-activation (<0 for false, >0 for true)\n",
    "    \n",
    "    ############## Log Test Info ##############\n",
    "    \n",
    "    # transform output to prediction in original image shape\n",
    "    output_predict = (output_up>=0)                             #(1,1,320,320) tensor True/False\n",
    "    output_up_predict_detached = output_predict.detach().cpu()  #(1,1,320,320) tensor True/False\n",
    "    output_up_predict_detached_numpy = np.squeeze(output_up_predict_detached.numpy().astype(np.float32)) #(1,1,320,320) np float 1/0\n",
    "    predicts = resize_and_crop(output_up_predict_detached_numpy, mask.shape[0], mask.shape[1]) #(425, 640) np float 32 0/1\n",
    "    \n",
    "    # Accuracy and IoU\n",
    "    acc, acc_pos, acc_neg = compute_accuracy_test(torch.from_numpy(predicts), torch.from_numpy(mask))\n",
    "    #iou, intersect, union = compute_iou_np(predicts, mask)\n",
    "    iou_down, intersect_down, union_down = compute_iou(output_up.detach().cpu(), mask_pass.detach().cpu())\n",
    "    \n",
    "    #IoU @ Precision\n",
    "    for i in range(len(iou_metrics)): # IoU @ Precision\n",
    "        iou_precision[i] += (iou_down >= iou_metrics[i])\n",
    "            \n",
    "    running_acc += acc\n",
    "    running_acc_pos += acc_pos\n",
    "    running_acc_neg += acc_neg\n",
    "    #running_miou += iou\n",
    "    #I += intersect\n",
    "    #U += union\n",
    "    running_miou += iou_down\n",
    "    down_I += intersect_down\n",
    "    down_U += union_down\n",
    "    \n",
    "    # Dense CRF post-processing\n",
    "    if opt.dcrf:\n",
    "        if opt.dataset == 'phrasecut':\n",
    "            predicts_dcrf = dcrf_calc(output_up.detach().cpu(), ubyte_im, opt.im_h, opt.im_w, opt.im_h, opt.im_w)\n",
    "        else:\n",
    "            predicts_dcrf = dcrf_calc(output_up.detach().cpu(), ubyte_im, opt.im_h, opt.im_w, mask.shape[0], mask.shape[1])\n",
    "        iou_dcrf, intersect_dcrf, union_dcrf = compute_iou_np(predicts_dcrf, mask_pass.detach().cpu().numpy()) #iou expects torch\n",
    "        running_miou_dcrf += iou_dcrf\n",
    "        I_dcrf += intersect_dcrf\n",
    "        U_dcrf += union_dcrf\n",
    "        for i in range(len(iou_metrics)): # IoU @ Precision\n",
    "            dcrf_iou_precision[i] += (iou_dcrf >= iou_metrics[i])\n",
    "    \n",
    "    # log test info\n",
    "    if iter % opt.test_log_every == 0 and iter != 0:\n",
    "        #calc\n",
    "        avg_miou = running_miou/(iter+1)\n",
    "        avg_overall_iou = (down_I/down_U)\n",
    "        avg_acc = running_acc/(iter+1)\n",
    "        avg_acc_pos = running_acc_pos/(iter+1)\n",
    "        avg_acc_neg = running_acc_neg/(iter+1)\n",
    "        \n",
    "        #log\n",
    "        opt.test_miou[iter] = avg_miou\n",
    "        opt.test_overall_iou[iter] = avg_overall_iou\n",
    "        opt.test_acc[iter] = avg_acc\n",
    "        opt.test_acc_pos[iter] = avg_acc_pos\n",
    "        opt.test_acc_neg[iter] = avg_acc_neg\n",
    "        opt.iou_precision = iou_precision\n",
    "        opt.dcrf_iou_precision = dcrf_iou_precision\n",
    "            \n",
    "        print('\\niter[%s]: mean_IoU=%.2f, Overall_IoU=%.4f' % (iter, avg_miou, avg_overall_iou))\n",
    "        print('acc_pos=%.2f, acc_neg=%.2f' % (avg_acc_pos, avg_acc_neg))\n",
    "        \n",
    "        if opt.dcrf:\n",
    "            avg_miou_dcrf = running_miou_dcrf/(iter+1)\n",
    "            avg_overall_iou_dcrf = I_dcrf/U_dcrf\n",
    "            opt.test_miou_dcrf[iter] = avg_miou_dcrf\n",
    "            opt.test_overall_iou_dcrf[iter] = avg_overall_iou_dcrf\n",
    "            print('DCRF: mean_IoU=%.4f, Overall_IoU=%.4f' % (avg_miou_dcrf, avg_overall_iou_dcrf))\n",
    "            \n",
    "        log_file = os.path.join(output_path + '/test.log')   \n",
    "        with open(log_file, 'w') as file:\n",
    "            for k, v in opt.items():\n",
    "                file.write(str(k) + '='+ str(v) + '\\n\\n')\n",
    "\n",
    "            \n",
    "    # Save image\n",
    "    if iter % opt.save_im_every == 0 and iter != 0:\n",
    "        image_display = torch.from_numpy(image) # (425, 640, 3) torch uint8\n",
    "        output_file_real = output_path + '/' + sentence + ' - ground truth.png'\n",
    "        output_file_pred = output_path + '/' + sentence + ' - predict truth.png'\n",
    "        visualize(torch.from_numpy(image), mask_pass.detach().cpu(), sentence + ' - ground truth', show=False, save=output_file_real)\n",
    "        visualize(torch.from_numpy(image), (output_up>=0).detach().cpu(), sentence + ' - prediction', show=False, save=output_file_pred)\n",
    "        \n",
    "        if opt.dcrf:\n",
    "            output_file_dcrf = output_path + '/' + sentence + ' - DCRF.png'\n",
    "            visualize(torch.from_numpy(image), torch.from_numpy(predicts_dcrf), sentence + ' - DCRF', show=False, save=output_file_dcrf)\n",
    "            \n",
    "        \n",
    "log_file = os.path.join(output_path + '/test.log')   \n",
    "with open(log_file, 'w') as file:\n",
    "    for k, v in opt.items():\n",
    "        file.write(str(k) + '='+ str(v) + '\\n\\n')\n",
    "        \n",
    "print('Done testing: Saved results at %s' % (log_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9CffjXmSI7g",
    "nbpresent": {
     "id": "dcda0d9c-507c-4288-a147-7c6f7dd04def"
    }
   },
   "source": [
    "# Test the output (for debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize(torch.from_numpy(image), mask_pass.detach().cpu(), sentence + ' - ground truth', show=True, save=None)\n",
    "visualize(torch.from_numpy(image), (output_up>=0).detach().cpu(), sentence + ' - prediction', show=True, save=None)\n",
    "visualize(torch.from_numpy(image), torch.from_numpy(predicts_dcrf), sentence + ' - DCRF', show=True, save=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GiMprgThJ0ts",
    "q9CffjXmSI7g"
   ],
   "name": "DL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "9ce2adf0-6da6-46c7-9b55-aefa66023a41",
    "theme": {
     "9ce2adf0-6da6-46c7-9b55-aefa66023a41": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "9ce2adf0-6da6-46c7-9b55-aefa66023a41",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         253,
         246,
         227
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         88,
         110,
         117
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         101,
         123,
         131
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b0e155c6c448909572e79ff8ab25d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25eec51aaabd48e2899878c6d81fcc5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "34eb3784f4664e15b22157c8f11303e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ec22ccfd84f49efae6fb4d450d2952e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b398441e11441f88a7e0b861353b05",
       "IPY_MODEL_a6448e2a7ba549dfbff4b24cbfe9d6bb"
      ],
      "layout": "IPY_MODEL_34eb3784f4664e15b22157c8f11303e3"
     }
    },
    "55b398441e11441f88a7e0b861353b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 79%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01b0e155c6c448909572e79ff8ab25d2",
      "max": 3000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25eec51aaabd48e2899878c6d81fcc5f",
      "value": 2380
     }
    },
    "71bd245539ea4537ba2f9ec1569f51c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6448e2a7ba549dfbff4b24cbfe9d6bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71bd245539ea4537ba2f9ec1569f51c6",
      "placeholder": "​",
      "style": "IPY_MODEL_eb27fc7f5c5c4f038cee808b41af255c",
      "value": " 2379/3000 [38:27&lt;10:00,  1.03it/s]"
     }
    },
    "eb27fc7f5c5c4f038cee808b41af255c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
