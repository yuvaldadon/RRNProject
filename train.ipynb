{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiMprgThJ0ts",
    "nbpresent": {
     "id": "cddedaf0-78f1-467a-a38e-d7f337017620"
    }
   },
   "source": [
    "# Imports and Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fX1daDvdptIN",
    "nbpresent": {
     "id": "45196f26-9fea-4224-8fda-c0779e3899c6"
    }
   },
   "outputs": [],
   "source": [
    "path = 'D:\\DLProject\\DL_turnin'\n",
    "output_path = 'D:\\DLProject\\DL_turnin\\output'\n",
    "\n",
    "%run tools/imports.py\n",
    "%run -i tools/functions.py\n",
    "%run -i tools/models.py\n",
    "%run -i tools/phrasecut.py\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy94hXcCJqA5",
    "nbpresent": {
     "id": "ad2a1e77-d04e-4a98-adf5-5251b31789b8"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "opt={}\n",
    "opt = dotdict(opt)\n",
    "\n",
    "## General\n",
    "opt.dataset = 'phrasecut'    # 'phrasecut'\n",
    "opt.split = 'train'          # 'train' 'test'\n",
    "opt.train_iter = 120000      # max iteration to train\n",
    "opt.train_log_every = 25000  # num of iterations to log training info\n",
    "opt.checkpoint_every = 300   # num of iterations to save checkpoint\n",
    "opt.load_checkpoint = None   # path to .pth to continue training\n",
    "opt.checkpoint = 'D:\\DLProject\\DL_turnin\\output\\checkpoint_125000.pth'  # path to .pth to continue training\n",
    "\n",
    "## Train Hyperparams\n",
    "opt.phrasecut_categories = ['c_coco'] # filter categories\n",
    "opt.new_img_proc = True               # use improved image processing (scaling -> normalize)\n",
    "opt.pos_weight = 2           # punish loss on positive labels\n",
    "opt.weight_decay = 0.0005    # Weight decay parameter (0.0005 => 0.005 output became too negative)\n",
    "opt.initial_lr = 0.00030     # initial learning rate (0.00025 => 0.00015)\n",
    "opt.min_lr = 0.00015         # minimum learning rate after decay\n",
    "opt.lr_decay = 0.9           # learning rate polyn decay rate\n",
    "opt.max_iter_lr = 500000     # iters to reach min_lr (should be around max_iter)\n",
    "opt.batch_size = 1           # Batch size (only support 1)\n",
    "opt.im_h = 320\n",
    "opt.im_w = 320\n",
    "opt.vf_h = 40\n",
    "opt.vf_w = 40\n",
    "opt.curr_iter = 0\n",
    "\n",
    "## Deeplab Hyperparams\n",
    "opt.base = 'v3'                  # backbone deeplab model\n",
    "opt.n_blocks = [3, 4, 23, 3]     # Resnet-101 layers\n",
    "opt.atrous_rates = [3, 6, 9, 12] # ASPP rates\n",
    "\n",
    "# RRN Hyperparams\n",
    "opt.embed_size = 1000        # embed vector size\n",
    "opt.hidden_size = 1000       # hidden layer size\n",
    "opt.num_layers = 1           # number of hidden layer\n",
    "opt.minval = -0.08\n",
    "opt.maxval = 0.08\n",
    "opt.num_steps = 20           # sentence length\n",
    "opt.rnn_size = 1000\n",
    "opt.mlp_dim = 500\n",
    "opt.vf_dim = 2048\n",
    "opt.scale_factor = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqTZ0X-Pr8Sh",
    "nbpresent": {
     "id": "f02db554-6ea0-4317-95c9-520e87f1ef5f"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3ec22ccfd84f49efae6fb4d450d2952e",
      "34eb3784f4664e15b22157c8f11303e3",
      "55b398441e11441f88a7e0b861353b05",
      "a6448e2a7ba549dfbff4b24cbfe9d6bb",
      "25eec51aaabd48e2899878c6d81fcc5f",
      "01b0e155c6c448909572e79ff8ab25d2",
      "eb27fc7f5c5c4f038cee808b41af255c",
      "71bd245539ea4537ba2f9ec1569f51c6"
     ]
    },
    "id": "ugLzp2FaqHC-",
    "nbpresent": {
     "id": "d6afc5c8-8ca5-4e7c-bf79-a41cbfba4171"
    },
    "outputId": "40eb12f5-e132-422c-f75b-25e0ff08931c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "opt.train_loss = {'iter': 'val'}\n",
    "opt.train_acc = {'iter': 'val'}\n",
    "opt.train_acc_pos = {'iter': 'val'}\n",
    "opt.train_acc_neg = {'iter': 'val'}\n",
    "opt.train_loss = {'iter': 'val'}\n",
    "opt.train_iou = {'iter': 'val'}\n",
    "opt.train_overall_iou = {'iter': 'val'}\n",
    "opt.vocab_size = 8407\n",
    "\n",
    "# Initialize Model\n",
    "model = Model(opt)\n",
    "\n",
    "# Load pre-trained model\n",
    "if opt.load_checkpoint:\n",
    "    checkpoint = torch.load(opt.checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    opt = checkpoint['opt']\n",
    "    opt = dotdict(opt)\n",
    "    \n",
    "    if opt.curr_iter >= opt.max_iter_lr:\n",
    "        lr = opt.min_lr\n",
    "    else:\n",
    "        lr = (opt.initial_lr - opt.min_lr) * ((1 - opt.curr_iter / opt.max_iter_lr) ** (opt.lr_decay)) + opt.min_lr\n",
    "else:\n",
    "    state_dict = torch.load(path + '/tools/deeplabv2_resnet101_msc-vocaug-20000.pth')\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    lr = opt.initial_lr\n",
    "    \n",
    "#Disable deeplab training except aspp\n",
    "for param in model.base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if opt.base == 'v3':\n",
    "    for param in model.base.aspp_v3.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# List of params to train\n",
    "parameters = []\n",
    "parameters.extend(model.layer5_feat.parameters())\n",
    "parameters.extend(model.LSTM.parameters())\n",
    "parameters.extend(model.RRN.parameters())\n",
    "if opt.base == 'v3':\n",
    "    parameters.extend(model.base.aspp_v3.parameters())\n",
    "\n",
    "# Criterion, Optimizer and lr decay\n",
    "pos_weight = torch.full([1], opt.pos_weight, dtype=torch.float32).to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
    "optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=opt.weight_decay)\n",
    "    \n",
    "# Load Optimizer\n",
    "if opt.load_checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "# activate GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == 'cuda':\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    \n",
    "# Data Loader\n",
    "refvg_loader = RefVGLoader(split=opt.split)\n",
    "img_ref_data = refvg_loader.get_img_ref_data()\n",
    "task_i = -2\n",
    "print('Loaded phrasecut: %s images, %s tasks' % (len(refvg_loader.img_ids), refvg_loader.task_num))\n",
    "\n",
    "# Init Vocab\n",
    "with open(str(dataset_dir) + '/name_att_rel_count.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "corpus = Corpus()\n",
    "corpus.split_and_add_words_to_vocab_from_data(data)\n",
    "\n",
    "image_batch = np.zeros((1, opt.im_h, opt.im_w, 3), dtype=np.float32)\n",
    "\n",
    "#im preprocess\n",
    "if opt.new_img_proc:\n",
    "    preprocess = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "else:\n",
    "    mu = np.array((104.00698793, 116.66876762, 122.67891434))\n",
    "\n",
    "# Acc and loss initialize\n",
    "running_acc, running_acc_pos, running_acc_neg, running_loss, running_iou = 0, 0, 0, 0, 0\n",
    "I, U = 0, 0\n",
    "n_batch, bw = 0, 0\n",
    "model.train()\n",
    "\n",
    "for iter in tq.tqdm(range(opt.curr_iter, opt.train_iter)):\n",
    "    \n",
    "    ############## Load Data ##############\n",
    "    \n",
    "    # Read next task\n",
    "    match = 0\n",
    "    while(match == 0):\n",
    "        task_i += 1\n",
    "        if (task_i >= len(img_ref_data['task_ids'])) or (task_i == -1):\n",
    "            img_ref_data = refvg_loader.get_img_ref_data()       # load img\n",
    "            img_id = img_ref_data['image_id']\n",
    "            img_p = str(img_fpath) + '/' + str(img_id) + '.jpg'  # (original shape) image\n",
    "            img = Image.open(img_p)\n",
    "            img = img.resize((opt.im_h, opt.im_w))               # (320, 320) image\n",
    "            image = np.array(img).astype(np.float32)             # (320, 320, 3) np float\n",
    "            task_i = 0\n",
    "            \n",
    "        # get task and categories\n",
    "        task_ids = img_ref_data['task_ids']\n",
    "        task = task_ids[task_i]\n",
    "        subsets_of_img = refvg_loader.get_task_subset(img_id, task)\n",
    "        \n",
    "        # Filter by category\n",
    "        for i in opt.phrasecut_categories:\n",
    "            if i in subsets_of_img:\n",
    "                match = 1\n",
    "                break\n",
    "\n",
    "    # extract phrase    \n",
    "    sentence = img_ref_data['phrases'][task_i]     # string 'large picture'\n",
    "    text_pass = corpus.tokenize_sentence(sentence).type(torch.LongTensor) #need to be (1,1,20) torch\n",
    "\n",
    "    # Ground truth mask\n",
    "    original_h = img_ref_data['height']            # 600\n",
    "    original_w = img_ref_data['width']             # 800\n",
    "    mask_up = np.zeros((original_h, original_w))                # (320,320) np\n",
    "    gt_Polygons = img_ref_data['gt_Polygons'][task_i]           # [plg0, plg1,..] for seperate objects\n",
    "    for plg in gt_Polygons:\n",
    "        mask_up += polygons_to_mask(plg, w=original_w, h=original_h) #(600,800) np 1/0\n",
    "    mask_up = torch.from_numpy(mask_up).unsqueeze(0).unsqueeze(0)    #(1,1,600,800) torch 1/0\n",
    "    #(1,1,320,320) torch 0/1\n",
    "    mask_pass = nn.functional.interpolate(mask_up, size=(320, 320), mode='bilinear', align_corners=False).to(device)\n",
    "\n",
    "    # Skip for black/white img\n",
    "    if len(image.shape) == 2:\n",
    "        bw += 1\n",
    "        continue\n",
    "\n",
    "    # processing image before pass\n",
    "    image_flip = image[:,:,0:3]         #rgba\n",
    "    image_flip = image_flip[:,:,::-1]   #(320, 320, 3)\n",
    "    if opt.new_img_proc != True:\n",
    "        image_flip -= mu\n",
    "\n",
    "    # add batch_size dimension\n",
    "    image_batch[n_batch, ...] = image_flip #(1, 320, 320, 3)\n",
    "\n",
    "    # turn into tensor\n",
    "    image_pass = torch.from_numpy(image_batch).permute(0,3,1,2).to(device) #(1,3,320,320) torch\n",
    "\n",
    "    # normalize [0,1] => normalize mean, std\n",
    "    if opt.new_img_proc:\n",
    "        image_pass = preprocess(image_pass.view(3,opt.im_h,opt.im_w)/255).view(1,3,opt.im_h,opt.im_w) #(1,3,320,320)\n",
    "        \n",
    "        \n",
    "    ############## Training Step ##############\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    output_down, output_up = model(image_pass, text_pass)  # output: (1,1,40,40), output_up: (1,1,320,320) pre-activation (<0 for false, >0 for true)\n",
    "    \n",
    "    # loss and backpass\n",
    "    loss = criterion(output_up,mask_pass) * opt.im_h * opt.im_w\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # learning rate decay\n",
    "    if iter >= opt.max_iter_lr:\n",
    "        lr = opt.min_lr\n",
    "    else:\n",
    "        lr = (opt.initial_lr - opt.min_lr) * ((1 - opt.curr_iter / opt.max_iter_lr) ** (opt.lr_decay)) + opt.min_lr\n",
    "        \n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    \n",
    "    ############## Log Training Info ##############\n",
    "    \n",
    "    # Accuracy and IoU\n",
    "    acc, acc_pos, acc_neg = compute_accuracy(output_up.detach().cpu(), mask_pass.detach().cpu())\n",
    "    iou, intersect, union = compute_iou(output_up.detach().cpu(), mask_pass.detach().cpu())\n",
    "    running_acc += acc\n",
    "    running_acc_pos += acc_pos\n",
    "    running_acc_neg += acc_neg\n",
    "    running_loss += loss.item()\n",
    "    running_iou += iou\n",
    "    I += intersect\n",
    "    U += union\n",
    "    \n",
    "    # log training info\n",
    "    if iter % opt.train_log_every == 0 and iter != 0:\n",
    "        avg_overall_iou = I/U\n",
    "        avg_iou = running_iou/(opt.train_log_every-bw)\n",
    "        avg_acc = running_acc/(opt.train_log_every-bw)\n",
    "        avg_acc_pos = running_acc_pos/(opt.train_log_every-bw)\n",
    "        avg_acc_neg = running_acc_neg/(opt.train_log_every-bw)\n",
    "        avg_loss = running_loss/(opt.train_log_every-bw)\n",
    "        opt.train_overall_iou[iter] = avg_overall_iou\n",
    "        opt.train_iou[iter] = avg_iou\n",
    "        opt.train_loss[iter] = avg_loss\n",
    "        opt.train_acc[iter] = avg_acc\n",
    "        opt.train_acc_pos[iter] = avg_acc_pos\n",
    "        opt.train_acc_neg[iter] = avg_acc_neg\n",
    "        print('\\niter[%s]: train_loss=%.2f, lr=%.5f' % (iter, avg_loss, optimizer.param_groups[0]['lr']))\n",
    "        print('mIoU=%.2f, overall_iou=%.2f, acc_pos=%.2f, acc_neg=%.2f' % (avg_iou, avg_overall_iou, avg_acc_pos, avg_acc_neg))\n",
    "        running_acc, running_acc_pos, running_acc_neg, running_loss, running_iou = 0, 0, 0, 0, 0\n",
    "        I, U = 0, 0\n",
    "        bw = 0\n",
    "\n",
    "            \n",
    "    # Save checkpoint at 'output_path/checkpoint_<iter>.pth'\n",
    "    if iter != 0 and (iter % opt.checkpoint_every == 0 or iter == (opt.train_iter-1)):\n",
    "        \n",
    "        # Save checkpoint at 'output_path/checkpoint_<iter>.pth'\n",
    "        checkpoint_file = os.path.join(output_path + '/checkpoint_' + str(iter) + '.pth')\n",
    "        opt.curr_iter = iter\n",
    "        checkpoint = {}\n",
    "        checkpoint['opt'] = opt.copy()\n",
    "        checkpoint['model'] = model.state_dict()\n",
    "        checkpoint['optimizer'] = optimizer.state_dict()\n",
    "        torch.save(checkpoint, checkpoint_file)\n",
    "        \n",
    "        # Save log at 'output_path/log_<iter>.log'\n",
    "        log_file = os.path.join(output_path + '/log_' + str(iter) + '.log')   \n",
    "        with open(log_file, 'w') as file:\n",
    "            for k, v in opt.items():\n",
    "                file.write(str(k) + '='+ str(v) + '\\n\\n')\n",
    "                \n",
    "        print('iter[%s]: Checkpoint at %s' % (iter, checkpoint_file))\n",
    "\n",
    "print('Done training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9CffjXmSI7g",
    "nbpresent": {
     "id": "dcda0d9c-507c-4288-a147-7c6f7dd04def"
    }
   },
   "source": [
    "# Test the output (for debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(torch.from_numpy(image), mask_pass.detach().cpu(), sentence + ' - ground truth', show=True, save=None)\n",
    "visualize(torch.from_numpy(image), (output_up>=0).detach().cpu(), sentence + ' - prediction', show=True, save=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GiMprgThJ0ts",
    "q9CffjXmSI7g"
   ],
   "name": "DL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "9ce2adf0-6da6-46c7-9b55-aefa66023a41",
    "theme": {
     "9ce2adf0-6da6-46c7-9b55-aefa66023a41": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "9ce2adf0-6da6-46c7-9b55-aefa66023a41",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         253,
         246,
         227
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         88,
         110,
         117
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         38,
         139,
         210
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         101,
         123,
         131
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01b0e155c6c448909572e79ff8ab25d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25eec51aaabd48e2899878c6d81fcc5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "34eb3784f4664e15b22157c8f11303e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ec22ccfd84f49efae6fb4d450d2952e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b398441e11441f88a7e0b861353b05",
       "IPY_MODEL_a6448e2a7ba549dfbff4b24cbfe9d6bb"
      ],
      "layout": "IPY_MODEL_34eb3784f4664e15b22157c8f11303e3"
     }
    },
    "55b398441e11441f88a7e0b861353b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 79%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01b0e155c6c448909572e79ff8ab25d2",
      "max": 3000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25eec51aaabd48e2899878c6d81fcc5f",
      "value": 2380
     }
    },
    "71bd245539ea4537ba2f9ec1569f51c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6448e2a7ba549dfbff4b24cbfe9d6bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71bd245539ea4537ba2f9ec1569f51c6",
      "placeholder": "​",
      "style": "IPY_MODEL_eb27fc7f5c5c4f038cee808b41af255c",
      "value": " 2379/3000 [38:27&lt;10:00,  1.03it/s]"
     }
    },
    "eb27fc7f5c5c4f038cee808b41af255c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
